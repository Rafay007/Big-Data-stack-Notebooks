{"cells":[{"cell_type":"code","source":["\n#getting the aws secret credentials\naccess_key = dbutils.secrets.get(scope = \"aws\", key = \"aws-access-key\")\nsecret_key = dbutils.secrets.get(scope = \"aws\", key = \"aws-secret-key\")\n\n#configuring aws\nhadoop_conf=spark._jsc.hadoopConfiguration()\nhadoop_conf.set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\nhadoop_conf.set(\"fs.s3n.awsAccessKeyId\", access_key)\nhadoop_conf.set(\"fs.s3n.awsSecretAccessKey\",secret_key)\nencoded_secret_key = secret_key.replace(\"/\", \"%2F\")\naws_bucket_name = \"raf2\"\nmount_name = \"aws-demo\"\n\n# If you are using Auto Loader file notification mode to load files, provide the AWS Region ID.\n# aws_region = \"aws-region-id\"\n# hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.\" + aws_region + \".amazonaws.com\")\n\n\n\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (access_key, encoded_secret_key, aws_bucket_name), \"/mnt/%s\" % mount_name)\ndisplay(dbutils.fs.ls(\"/mnt/%s\" % mount_name))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32ab3949-ed6c-4329-831c-db27ac4d081d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">s3.ObjectSummary(bucket_name=&#39;raf2&#39;, key=&#39;folder1/&#39;)\ns3.ObjectSummary(bucket_name=&#39;raf2&#39;, key=&#39;folder1/Sample-Spreadsheet-10000-rows.csv&#39;)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">s3.ObjectSummary(bucket_name=&#39;raf2&#39;, key=&#39;folder1/&#39;)\ns3.ObjectSummary(bucket_name=&#39;raf2&#39;, key=&#39;folder1/Sample-Spreadsheet-10000-rows.csv&#39;)\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["Enabling spark.eventLog.rolling.enabled and spark.eventLog.rolling.maxFileSize would let you have rolling event log files instead of single huge event log file which may help some scenarios on its own, but it still doesn’t help you reducing the overall size of logs.\n\nSpark History Server can apply compaction on the rolling event log files to reduce the overall size of logs, via setting the configuration spark.history.fs.eventLog.rolling.maxFilesToRetain on the Spark History Server.\n\nOfficial page: https://spark.apache.org/docs/latest/monitoring.html "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0141a7d4-fd9d-43d9-bfb2-51c3b7d22e5d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Kafka to Snowflake through spark\") \\\n    .config(\"spark.python.profile\", \"true\") \\\n    .config(\"spark.shuffle.compress\", \"true\") \\\n    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n    .config(\"spark.shuffle.io.backLog\", \"2\") \\\n    .config(\"spark.shuffle.service.enabled\", \"true\") \\\n    .config(\"spark.eventLog.enabled\", \"true\") \\\n    .config(\"spark.eventLog.rolling.enabled\", \"true\") \\# \n    .config(\"spark.eventLog.rolling.maxFileSize\", \"10m\") \\\n    .config(\"spark.eventLog.dir\", \"/mnt/%s\" % mount_name) \\#saving the spark logs to S3 to visualize the DAGS later on\n    .getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Setting essential spark configs","showTitle":true,"inputWidgets":{},"nuid":"197ded4c-c578-4d72-8d43-1cd16b8ab029"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df=spark.read.csv(\"dbfs:/mnt/%s/...\" % mount_name, header=True)\ndf.cache().count()\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Reading and printing data for just checking","showTitle":true,"inputWidgets":{},"nuid":"65f47dfd-81ea-49d5-ac98-eaccad454069"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---+------------------------------------------------+------------------+----+-------+------+-----+-------+----------------------+----+\n|  1|Eldon Base for stackable storage shelf, platinum|Muhammed MacIntyre|   3|-213.25| 38.94|   35|Nunavut|Storage &amp; Organization| 0.8|\n+---+------------------------------------------------+------------------+----+-------+------+-----+-------+----------------------+----+\n|  2|                            &#34;1.7 Cubic Foot C...|      Barry French| 293| 457.81|208.16|68.02|Nunavut|            Appliances|0.58|\n|  3|                            Cardinal Slant-D�...|      Barry French| 293|  46.71|  8.69| 2.99|Nunavut|  Binders and Binde...|0.39|\n|  4|                                            R380|     Clay Rozendal| 483|1198.97|195.99| 3.99|Nunavut|  Telephones and Co...|0.58|\n|  5|                            Holmes HEPA Air P...|    Carlos Soltero| 515|  30.94| 21.78| 5.94|Nunavut|            Appliances| 0.5|\n|  6|                            G.E. Longer-Life ...|    Carlos Soltero| 515|   4.43|  6.64| 4.95|Nunavut|    Office Furnishings|0.37|\n|  7|                            Angle-D Binders w...|      Carl Jackson| 613| -54.04|   7.3| 7.72|Nunavut|  Binders and Binde...|0.38|\n|  8|                            SAFCO Mobile Desk...|      Carl Jackson| 613| 127.70| 42.76| 6.22|Nunavut|  Storage &amp; Organiz...|null|\n|  9|                            SAFCO Commercial ...|    Monica Federle| 643|-695.26|138.14|   35|Nunavut|  Storage &amp; Organiz...|null|\n| 10|                                       Xerox 198|   Dorothy Badders| 678|-226.36|  4.98| 8.33|Nunavut|                 Paper|0.38|\n| 11|                                      Xerox 1980|   Neola Schneider| 807|-166.85|  4.28| 6.18|Nunavut|                 Paper| 0.4|\n| 12|                            Advantus Map Penn...|   Neola Schneider| 807| -14.33|  3.95|    2|Nunavut|          Rubber Bands|0.53|\n| 13|                            Holmes HEPA Air P...|       Carlos Daly| 868| 134.72| 21.78| 5.94|Nunavut|            Appliances| 0.5|\n| 14|                            DS/HD IBM Formatt...|       Carlos Daly| 868| 114.46| 47.98| 3.61|Nunavut|  Computer Peripherals|0.71|\n| 15|                            &#34;Wilson Jones 1&#34;&#34;...|     Claudia Miner| 933|  -4.72|  5.28| 2.99|Nunavut|  Binders and Binde...|0.37|\n| 16|                            Ultra Commercial ...|   Neola Schneider| 995| 782.91| 39.89| 3.04|Nunavut|    Office Furnishings|0.53|\n| 17|                            &#34;#10-4 1/8&#34;&#34; x 9 ...|  Allen Rosenblatt| 998|  93.80| 15.74| 1.39|Nunavut|             Envelopes| 0.4|\n| 18|                            Hon 4-Shelf Metal...|   Sylvia Foulston|1154| 440.72|100.98|26.22|Nunavut|             Bookcases| 0.6|\n| 19|                            Lesro Sheffield C...|   Sylvia Foulston|1154|-481.04| 71.37|   69|Nunavut|                Tables|0.68|\n| 20|                                            g520|       Jim Radford|1344| -11.68| 65.99| 5.26|Nunavut|  Telephones and Co...|0.59|\n| 21|                                          LX 788|       Jim Radford|1344| 313.58|155.99| 8.99|Nunavut|  Telephones and Co...|0.58|\n+---+------------------------------------------------+------------------+----+-------+------+-----+-------+----------------------+----+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+------------------------------------------------+------------------+----+-------+------+-----+-------+----------------------+----+\n  1|Eldon Base for stackable storage shelf, platinum|Muhammed MacIntyre|   3|-213.25| 38.94|   35|Nunavut|Storage &amp; Organization| 0.8|\n+---+------------------------------------------------+------------------+----+-------+------+-----+-------+----------------------+----+\n  2|                            &#34;1.7 Cubic Foot C...|      Barry French| 293| 457.81|208.16|68.02|Nunavut|            Appliances|0.58|\n  3|                            Cardinal Slant-D�...|      Barry French| 293|  46.71|  8.69| 2.99|Nunavut|  Binders and Binde...|0.39|\n  4|                                            R380|     Clay Rozendal| 483|1198.97|195.99| 3.99|Nunavut|  Telephones and Co...|0.58|\n  5|                            Holmes HEPA Air P...|    Carlos Soltero| 515|  30.94| 21.78| 5.94|Nunavut|            Appliances| 0.5|\n  6|                            G.E. Longer-Life ...|    Carlos Soltero| 515|   4.43|  6.64| 4.95|Nunavut|    Office Furnishings|0.37|\n  7|                            Angle-D Binders w...|      Carl Jackson| 613| -54.04|   7.3| 7.72|Nunavut|  Binders and Binde...|0.38|\n  8|                            SAFCO Mobile Desk...|      Carl Jackson| 613| 127.70| 42.76| 6.22|Nunavut|  Storage &amp; Organiz...|null|\n  9|                            SAFCO Commercial ...|    Monica Federle| 643|-695.26|138.14|   35|Nunavut|  Storage &amp; Organiz...|null|\n 10|                                       Xerox 198|   Dorothy Badders| 678|-226.36|  4.98| 8.33|Nunavut|                 Paper|0.38|\n 11|                                      Xerox 1980|   Neola Schneider| 807|-166.85|  4.28| 6.18|Nunavut|                 Paper| 0.4|\n 12|                            Advantus Map Penn...|   Neola Schneider| 807| -14.33|  3.95|    2|Nunavut|          Rubber Bands|0.53|\n 13|                            Holmes HEPA Air P...|       Carlos Daly| 868| 134.72| 21.78| 5.94|Nunavut|            Appliances| 0.5|\n 14|                            DS/HD IBM Formatt...|       Carlos Daly| 868| 114.46| 47.98| 3.61|Nunavut|  Computer Peripherals|0.71|\n 15|                            &#34;Wilson Jones 1&#34;&#34;...|     Claudia Miner| 933|  -4.72|  5.28| 2.99|Nunavut|  Binders and Binde...|0.37|\n 16|                            Ultra Commercial ...|   Neola Schneider| 995| 782.91| 39.89| 3.04|Nunavut|    Office Furnishings|0.53|\n 17|                            &#34;#10-4 1/8&#34;&#34; x 9 ...|  Allen Rosenblatt| 998|  93.80| 15.74| 1.39|Nunavut|             Envelopes| 0.4|\n 18|                            Hon 4-Shelf Metal...|   Sylvia Foulston|1154| 440.72|100.98|26.22|Nunavut|             Bookcases| 0.6|\n 19|                            Lesro Sheffield C...|   Sylvia Foulston|1154|-481.04| 71.37|   69|Nunavut|                Tables|0.68|\n 20|                                            g520|       Jim Radford|1344| -11.68| 65.99| 5.26|Nunavut|  Telephones and Co...|0.59|\n 21|                                          LX 788|       Jim Radford|1344| 313.58|155.99| 8.99|Nunavut|  Telephones and Co...|0.58|\n+---+------------------------------------------------+------------------+----+-------+------+-----+-------+----------------------+----+\nonly showing top 20 rows\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as spark_func\n\nspark.conf.set(\"spark.sql.shuffle.partitions\",spark.sparkContext.defaultParallelism)\n\n# Databricks provide the full fledged Kafka Server in which the Wikipedia edits are being written in real time from the various language-specific IRC channels to which Wikipedia posts them.\n# The Kafka server parses the IRC data and then converts the messages to JSON format, and sends the JSON to a Kafka server that has a retention period of 3 days\n# We will just consume those messages from Kafka already set by Databricks for learning\nKafka_server = \"server1.databricks.training:9092\"\nkafka_properties = {\n  'kafka.bootstrap.servers': Kafka_server,\n  'subscribe':'en',# here we are subscribing to english topic that is from en.wikipedia.org\n  'startingOffsets':'earliest',\n  'maxOffsetsPerTrigger':1000\n}\n\n# reading the stream data from Kafka\ndf = (spark.readStream\n      .format('kafka')\n      .options(**kafka_properties)\n      .load()\n      .select(spark_func.col('value').cast('string'))# we are selecting the data that is in json\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60d0f54b-14ec-4440-bbf8-bbc28d08f4a3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# the kafka topic \"en\" has the following fields\n(spark.readStream\n      .format('kafka')\n      .options(**kafka_properties)\n      .load())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"393c1bb8-cbf1-4526-a2f3-7a835a3692b5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[22]: DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[22]: DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.types as spark_type\n\n#setting up the \nschema = spark_type.StructType([\n  spark_type.StructField('channel',spark_type.StringType(),True),\n  spark_type.StructField('comment',spark_type.StringType(),True),\n  spark_type.StructField('delta',spark_type.IntegerType(),True),\n  spark_type.StructField('flag',spark_type.StringType(),True),\n  spark_type.StructField(\"geocoding\",spark_type.StructType([\n    spark_type.StructField('city',spark_type.StringType(),True),\n    spark_type.StructField('country',spark_type.StringType(),True),\n    spark_type.StructField('countrycode2',spark_type.StringType(),True),\n    spark_type.StructField('countrycode3',spark_type.StringType(),True),\n    spark_type.StructField('stateProvince',spark_type.StringType(),True),\n    spark_type.StructField('latitude',spark_type.DoubleType(),True),\n    spark_type.StructField('longitude',spark_type.DoubleType(),True),\n  ]),True),\n  spark_type.StructField('isAnonymous',spark_type.BooleanType(),True),\n  spark_type.StructField('isNewPage',spark_type.BooleanType(),True),\n  spark_type.StructField('isRobot',spark_type.BooleanType(),True),\n  spark_type.StructField('isUnpatrolled',spark_type.BooleanType(),True),\n  spark_type.StructField('namespace',spark_type.StringType(),True),\n  spark_type.StructField('page',spark_type.StringType(),True),\n  spark_type.StructField('pageURL',spark_type.StringType(),True),\n  spark_type.StructField('timestamp',spark_type.StringType(),True),\n  spark_type.StructField('url',spark_type.StringType(),True),\n  spark_type.StructField('user',spark_type.StringType(),True),\n  spark_type.StructField('userURL',spark_type.StringType(),True),\n  spark_type.StructField('wikipediaURL',spark_type.StringType(),True),\n  spark_type.StructField('wikipedia',spark_type.StringType(),True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66089109-7b8b-4cac-a85b-4e49fee5e730"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["new_df = df.select(spark_func.from_json('value',schema).alias('json'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec0acf62-85da-4255-897f-72319a4c7ada"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# getting the nested fields of json and some basic spark filtering\nnew_df2 = (\n           new_df\n  .select(\n    spark_func.col('json.channel').alias('channel'),\n    spark_func.col('json.comment').alias('comment'),\n    spark_func.col('json.delta').alias('delta'),\n    spark_func.col('json.flag').alias('flag'),\n    spark_func.col('json.geocoding').alias('geocoding'),\n    spark_func.col('json.isAnonymous').alias('isAnonymous'),\n    spark_func.col('json.isNewPage').alias('isNewPage'),\n    spark_func.col('json.isRobot').alias('isRobot'),\n    spark_func.col('json.user').alias('user'),\n    spark_func.col('json.wikipediaURL').alias('wikipediaURL'),\n    spark_func.col('json.wikipedia').alias('wikipedia')\n  \n  )\n  .filter(spark_func.col('json.namespace')=='article')\n  .filter(spark_func.col('json.namespace').isNotNull())\n          \n          )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"678334f3-5ae9-46b9-bfbd-dff7ff007e01"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#getting the aws secret credentials\nsfUrl = dbutils.secrets.get(scope = \"SFUrl\", key = \"sfUrl\")\nsfPassword = dbutils.secrets.get(scope = \"SFPass\", key = \"sfPassword\")\n\ncredential_properties = {\n  'sfUrl':sfUrl,\n  'sfUser':'Rafay007',\n  'sfPassword':sfPassword,\n  'sfDatabase':'CITIBIKE_rafay',\n  'sfSchema':'PUBLIC',\n  'sfWarehouse':'COMPUTE_WH'\n  \n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6827110a-8361-482e-b8e6-c398a3f57e06"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["stream_name = 'KafkaStream'\ntable_name = 'kafka_wiki'\n\ndef writeToSnowflake(df,epochId):\n  print(epochId)\n  df.write.format('snowflake').options(**credential_properties).option('dbtable',table_name).mode('append').save()\n  \nif __name__=='__main__':\n  \n  StremProcessing = (\n    new_df2.writeStream\n    .queryName(stream_name)\n    .trigger(processingTime='3 seconds')\n    .foreachBatch(writeToSnowflake)\n    .start()\n\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69f6d1f8-4a4b-4227-b885-258770d106ea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Kafka-to-spark-to-snowflake","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3978595834561856}},"nbformat":4,"nbformat_minor":0}
